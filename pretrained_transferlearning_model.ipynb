{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/jherberg462/SIIM-ISIC.git --depth 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "try:\n",
    "    from kaggle_datasets import KaggleDatasets\n",
    "    dataset_gcs = KaggleDatasets().get_gcs_path('siim-isic-melanoma-classification')\n",
    "    print('got GCS path via KaggleDatasets .get_gcs_path method')\n",
    "except ModuleNotFoundError:\n",
    "    #hardcode path when running outside of Kaggle\n",
    "    dataset_gcs = 'gs://kds-1e4e2af3c4717d1070d563d5555e3e09bd7df325fccbfa453ee9b532'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './SIIM-ISIC')\n",
    "from input_pipeline import (decode_image_label, decode_image, \n",
    "                            normalize_image_label, random_flip, \n",
    "                            get_train_ds, get_test_ds, get_ds_size)\n",
    "from params import params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['batch_size'] = params['batch_size'] * strategy.num_replicas_in_sync\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(dataset_gcs + '/train.csv')\n",
    "train_df.groupby('target').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_pretrained(input_shape=[*params['img_size'], 3], bias_output=None):\n",
    "    '''\n",
    "    function to create a model that will be used to train DS via xfer learning\n",
    "    \n",
    "    args:\n",
    "        input_shape: array, default: [1024, 1024, 3], shape\n",
    "            of input tensor that will be fed into model\n",
    "    \n",
    "    returns:\n",
    "        model: keras.Model() model\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    input_tensor = layers.Input(shape=input_shape, name='images_input')\n",
    "    \n",
    "\n",
    "    \n",
    "    #s/b 299, 299, 3 // needs to be -1 --> 1\n",
    "    xception = tf.keras.applications.Xception(include_top=False,\n",
    "                                                       input_shape=input_shape,\n",
    "                                                       classes=1)(input_tensor)\n",
    "    end_of_xception = layers.GlobalAveragePooling2D()(xception)\n",
    "    dense_layers = layers.Dense(8)(end_of_xception)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if bias_output is not None:\n",
    "        bias_output = tf.keras.initializers.Constant(bias_output)\n",
    "    output = layers.Dense(1, activation='sigmoid', bias_initializer=bias_output)(dense_layers)\n",
    "    model = keras.Model(inputs=input_tensor, outputs=output)\n",
    "    \n",
    "    \n",
    "    metrics = [\n",
    "          keras.metrics.TruePositives(name='tp'),\n",
    "          keras.metrics.FalseNegatives(name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "    ]\n",
    "    schedule = None\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.01),\n",
    "        metrics=metrics)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get test file paths\n",
    "test_files = tf.io.gfile.glob(dataset_gcs + '/tfrecords/test*.tfrec')\n",
    "\n",
    "#get train and validation file paths\n",
    "train_files, valid_files = train_test_split(tf.io.gfile.glob(dataset_gcs + '/tfrecords/train*.tfrec'),\n",
    "                              test_size=.1, random_state=1)\n",
    "\n",
    "#create datasets\n",
    "train_ds = get_train_ds(train_files, params['batch_size'])\n",
    "valid_ds = get_train_ds(valid_files, params['batch_size'])\n",
    "test_ds = get_test_ds(test_files, params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, valid_size = get_ds_size(train_files), get_ds_size(valid_files)\n",
    "test_size = get_ds_size(test_files)\n",
    "print('the dataset consists of: {} training images, {} validation images, and {} test images'.\n",
    "     format(train_size, valid_size, test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_steps = train_size / params['batch_size'] \n",
    "valid_steps = valid_size / params['batch_size']\n",
    "test_steps = 1.0 * test_size / params['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate class weights\n",
    "\n",
    "targets = train_df.groupby('target').count()['diagnosis'].to_list()\n",
    "target_0 = targets[0]\n",
    "target_1 = targets[1]\n",
    "total = target_0 + target_1\n",
    "\n",
    "class_weight_0 = (1 / target_0) * (total) / 2.0\n",
    "class_weight_1 = (1 / target_1) * (total) / 2.0\n",
    "\n",
    "class_weights = {0: class_weight_0, 1: class_weight_1}\n",
    "\n",
    "initial_bias = np.log([target_1 / target_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = create_model_pretrained(bias_output=initial_bias)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_auc', #val_auc\n",
    "                                patience=25,\n",
    "                                mode='max',\n",
    "                                restore_best_weights=True)\n",
    "\n",
    "def lr_schedule_fn(epoch, lr):\n",
    "    if epoch < 8:\n",
    "        return 0.000001\n",
    "    elif epoch == 8:\n",
    "        return 0.001\n",
    "    elif epoch %2 ==0 and epoch < 49:\n",
    "        return lr * 0.75\n",
    "    else:\n",
    "        return lr\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lr_schedule_fn)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs= params['epochs'], \n",
    "    steps_per_epoch=epoch_steps,\n",
    "    validation_data=valid_ds,\n",
    "    validation_steps=valid_steps,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stopping, lr_schedule],\n",
    "    verbose = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_ds.map(lambda img, igs: img), steps=test_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_ids = next(iter(test_ds.\n",
    "                          map(lambda img, ids:ids).\n",
    "                          unbatch().\n",
    "                          batch(test_size))).numpy().astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dict = {\n",
    "    'image_name': prediction_ids,\n",
    "    'target': np.concatenate(predictions)\n",
    "}\n",
    "submission_ds = pd.DataFrame(prediction_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_ds.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_ds.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(history_, metric1, metric2, ylabel):\n",
    "    plt.plot(history_.history[metric1], label=metric1)\n",
    "    plt.plot(history_.history[metric2], label=metric2)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history, 'auc', 'val_auc', 'auc')\n",
    "plot_metric(history, 'loss', 'val_loss', 'loss')\n",
    "plot_metric(history, 'accuracy', 'val_accuracy', 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
